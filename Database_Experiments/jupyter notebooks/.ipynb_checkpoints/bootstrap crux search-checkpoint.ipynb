{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, shutil\n",
    "\n",
    "'''__get_related_files\n",
    "\n",
    "DESC:\n",
    "    Given some substring, return all files with that substring\n",
    "PARAMS:\n",
    "    files: list of strings of file names\n",
    "    sub: string substring to find in the files\n",
    "OPTIONAL: \n",
    "    not_sub: string a substring that if found in file name don't add\n",
    "'''\n",
    "def __get_related_files(files, sub, not_sub=None):\n",
    "    if not_sub is not None and not_sub != '':\n",
    "        return [x for x in files if sub in x and not_sub not in x]\n",
    "    return [x for x in files if sub in x]\n",
    "\n",
    "'''__make_valid_dir_string\n",
    "\n",
    "DESC:\n",
    "    add / to end of director string if it doesn't have it alread\n",
    "PARAMS:\n",
    "    dir_path: string path to directory\n",
    "RETURNS:\n",
    "    dir path with / at end\n",
    "'''\n",
    "def __make_valid_dir_string(dir_path):\n",
    "    return dir_path + '/' if dir_path[-1] != '/' else dir_path\n",
    "\n",
    "'''__make_dir\n",
    "\n",
    "DESC:\n",
    "    Given a path to directory, check if it exists and if not create it\n",
    "PARAMS:\n",
    "    dir_path: string path to a directory to make or check\n",
    "RETURNS:\n",
    "    None\n",
    "'''\n",
    "def __make_dir(dir_path):\n",
    "    dir_path = __make_valid_dir_string(dir_path)\n",
    "    if not os.path.exists(dir_path): \n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "'''__make_valid_text_file\n",
    "\n",
    "DESC:\n",
    "    make a string into the name for a text file and make sure directory exists\n",
    "PARAMS:\n",
    "    file_name: string a name of the file to save\n",
    "RETURNS:\n",
    "    file name with .txt after\n",
    "'''\n",
    "def __make_valid_text_file(file_name):\n",
    "    file_name = file_name + '.txt' if '.txt' not in file_name else file_name\n",
    "    return file_name\n",
    "\n",
    "'''__make_valid_json_file\n",
    "\n",
    "DESC:\n",
    "    make a string into the name for a text file and make sure the directory exists\n",
    "PARAMS:\n",
    "    file_name: string of the file to save\n",
    "RETURNS:\n",
    "    file name with .json after it\n",
    "'''\n",
    "def __make_valid_json_file(file_name):\n",
    "    file_name = file_name + '.json' if '.json' not in file_name else file_name\n",
    "    return file_name\n",
    "\n",
    "'''__make_valid_csv_file\n",
    "\n",
    "DESC:\n",
    "    make a string into the name for a text file \n",
    "PARAMS:\n",
    "    file_name: string of the file to save\n",
    "RETURNS:\n",
    "    file name with .csv after it\n",
    "'''\n",
    "def __make_valid_csv_file(file_name):\n",
    "    file_name = file_name + '.csv' if '.csv' not in file_name else file_name\n",
    "    return file_name\n",
    "\n",
    "'''__make_valid_fasta_file\n",
    "\n",
    "DESC:\n",
    "    make a string into the name for a text file \n",
    "PARAMS:\n",
    "    file_name: string of the file to save\n",
    "RETURNS:\n",
    "    file name with .fasta after it\n",
    "'''\n",
    "def __make_valid_fasta_file(file_name):\n",
    "    file_name = file_name + '.fasta' if '.fasta' not in file_name else file_name\n",
    "    return file_name\n",
    "\n",
    "'''__file_exists\n",
    "\n",
    "DESC:\n",
    "    find out if a file exists\n",
    "PARAMS:\n",
    "    file_name: string name of the file to check for\n",
    "RETURNS:\n",
    "    bool true if file exists false otherwise\n",
    "'''\n",
    "def __file_exists(file_name):\n",
    "    return os.path.isfile(file_name)\n",
    "\n",
    "'''__gzip\n",
    "\n",
    "DESC:\n",
    "    zip up a file\n",
    "PARAMS: \n",
    "    file_name: str path to the file name to compress\n",
    "OPTIONAL:\n",
    "    delete_old: bool delete the uncompressed file. Default=True\n",
    "RETURNS:\n",
    "    str name of the new compressed file\n",
    "'''\n",
    "def __gzip(file_name, delete_old=True):\n",
    "    compressed_file_name = file_name + '.gz'\n",
    "    with open(file_name, 'rb') as f_in:\n",
    "        with gzip.open(compressed_file_name, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    delete_old and os.remove(file_name)\n",
    "    return compressed_file_name\n",
    "\n",
    "'''__gunzip\n",
    "\n",
    "DESC:   \n",
    "    unzip a file\n",
    "PARAMS:\n",
    "    compressed_file_name: str name of the compressed file to unzip\n",
    "OPTIONAL:\n",
    "    delete_old: bool delete the compressed file. Default=True\n",
    "RETURNS:\n",
    "    str name of the file unziped\n",
    "'''  \n",
    "def __gunzip(compressed_file_name, delete_old=True):\n",
    "    file_name = compressed_file_name if '.gz' not in compressed_file_name else compressed_file_name.replace('.gz', '')\n",
    "    with gzip.open(compressed_file_name, 'rb') as f_in:\n",
    "        with open(file_name, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    delete_old and os.remove(compressed_file_name)\n",
    "    return file_name\n",
    "\n",
    "'''__is_gzipped\n",
    "\n",
    "DESC:\n",
    "    determines if a file has been gzipped\n",
    "PARAMS:\n",
    "    file_name: str path to file in question\n",
    "RETURNS:\n",
    "    bool True if file is compressed else False\n",
    "'''\n",
    "def __is_gzipped(file_name):\n",
    "    return '.gz' == file_name[-3:]\n",
    "\n",
    "'''__gzip_dir\n",
    "\n",
    "DESC:\n",
    "    compress a directory with gzip\n",
    "PARAMS:\n",
    "    d: str path to directory\n",
    "OPTIONAL:\n",
    "    delete_old: bool delete the unziped directory. Default=True\n",
    "RETURNS:\n",
    "    path to the new zipped folder\n",
    "'''\n",
    "def __gzip_dir(d, delete_old=True):\n",
    "    root = '/'.join(d.split('/')[:-1])\n",
    "    shutil.make_archive(d, 'zip', root)\n",
    "    delete_old and shutil.rmtree(d)\n",
    "    return d + '.zip'\n",
    "\n",
    "'''__is_json\n",
    "\n",
    "DESC:\n",
    "    determine if a file is a json file based purely on name\n",
    "PARAMS:\n",
    "    file: file to determine if its a json file\n",
    "RETURNS:\n",
    "    bool True if it is a json file False otherwise\n",
    "'''\n",
    "def __is_json(file):\n",
    "    return True if '.json' in file else False\n",
    "\n",
    "'''__is_fasta\n",
    "\n",
    "DESC:\n",
    "    determine if a file is a fasta file based purely on name\n",
    "PARAMS:\n",
    "    file: file to determine if its a fasta file\n",
    "RETURNS:\n",
    "    bool True if it is a fasta file False otherwise\n",
    "'''\n",
    "def __is_fasta(file):\n",
    "    return True if '.fasta' in file else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_experiment(f):\n",
    "    return json.load(open(f, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectra generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids={\n",
    "    \"A\":71.037114,\n",
    "    \"R\":156.101111,\n",
    "    \"N\":114.042927,\n",
    "    \"D\":115.026943,\n",
    "    \"C\":103.009185,\n",
    "    \"E\":129.042593,\n",
    "    \"Q\":128.058578,\n",
    "    \"G\":57.021464,\n",
    "    \"H\":137.058912,\n",
    "    \"I\":113.084064,\n",
    "    \"L\":113.084064,\n",
    "    \"K\":128.094963,\n",
    "    \"M\":131.040485,\n",
    "    \"F\":147.068414,\n",
    "    \"P\":97.052764,\n",
    "    \"S\":87.032028,\n",
    "    \"T\":101.047679,\n",
    "    \"U\":150.95363,\n",
    "    \"W\":186.079313,\n",
    "    \"Y\":163.06332,\n",
    "    \"V\":99.068414\n",
    "}\n",
    "\n",
    "'''__calc_masses\n",
    "\n",
    "DESC:\n",
    "    calculates the masses/spectrum for a sequence\n",
    "PARAMS:\n",
    "    sequence: str amino acid sequence to change to list of masses\n",
    "    charge: int charge value to calculate masses for\n",
    "RETURNS:\n",
    "    list of floats, float       spectrum and the precursor mass \n",
    "'''\n",
    "def __calc_masses(sequence, charge):\n",
    "    masses = []\n",
    "\n",
    "    length = len(sequence)\n",
    "    total = 2 * 1.007825035 + 15.99491463 #This is the mass of water. Adding the mass of water to the sum of all the residue masses gives the mass of the peptide.\n",
    "    for i in range(length):\n",
    "        total +=  amino_acids[sequence[i]]\n",
    "\n",
    "    pre_mz = (total+charge*1.0072764)/charge   \n",
    "\n",
    "    if charge == 1:\n",
    "        #b+\n",
    "        total = 1.007825035 - 0.0005486 #for the H to turn the residue NH on the N-terminus into NH2\n",
    "        for i in range (0, length):\n",
    "            total += amino_acids[sequence[i]]\n",
    "            masses.append(total)\n",
    "            #Since z (the charge) is equal to one, the total here is the m/z\n",
    "\n",
    "        #y+\n",
    "        total = 3 * 1.007825035 + 15.99491463 - 0.0005486 #for the OH to turn the residue CO on the C-terminus into COOH + 1 proton to make NH into NH2 and 1 proton make positively charged\n",
    "        for i in range (0,length):\n",
    "            total += amino_acids[sequence[length-i-1]]\n",
    "            masses.append(total)\n",
    "\n",
    "    elif charge == 2:\n",
    "        #b++\n",
    "        total = 2 * 1.007825035 - 2 * 0.0005486 #adding one more proton this time to make it doubly charged\n",
    "        for i in range (0, length):\n",
    "            total += amino_acids[sequence[i]]\n",
    "            masses.append(total/2)\n",
    "\n",
    "        #y++\n",
    "        total = 4 * 1.007825035 + 15.99491463 - 2 * 0.0005486 #another proton to make doubly charged\n",
    "        for i in range (0, length):\n",
    "            total += amino_acids[sequence[length-i-1]]\n",
    "            masses.append(total/2)\n",
    "        #The masses you get exactly match Spectrum Mill. To get this, I had to make sure to use the mass of H+ and the mass of H when appropriate.\n",
    "\n",
    "    return masses, pre_mz\n",
    "\n",
    "'''gen_spectra\n",
    "\n",
    "DESC:\n",
    "    generates mass spectra for sequences\n",
    "PARAMS:\n",
    "    sequences: list of strings sequences to generate spectra for\n",
    "RETURNS:\n",
    "    list of dictionaries of the form {'spectrum': list of floats, 'precursor_mass': float}\n",
    "'''\n",
    "def gen_spectra(sequenences):\n",
    "    spectra = []\n",
    "    for sequence in sequenences:\n",
    "        this_entry = {}\n",
    "        mass_1, _ = __calc_masses(sequence, 1)\n",
    "        mass_2, pre_mz = __calc_masses(sequence, 2)\n",
    "        this_spectra = mass_1 + mass_2\n",
    "        this_spectra.sort()\n",
    "        this_entry['spectrum'] = this_spectra\n",
    "        this_entry['precursor_mass'] = pre_mz\n",
    "        spectra.append(this_entry)\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectra writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Determination of memory status is not supported on this \n",
      " platform, measuring for memoryleaks will never fail\n"
     ]
    }
   ],
   "source": [
    "from pyopenms import MSExperiment, MSSpectrum, MzMLFile, Peak1D, Precursor\n",
    "\n",
    "'''write_mzml\n",
    "\n",
    "DESC:\n",
    "    create a mass spectrum file in mzml of sequences\n",
    "PARAMS:\n",
    "    file_name: str name to save the file in\n",
    "    spectra: list of dictionaries of the form [{spectrum: list[floats], precursor_mass: float, sequence: str}]\n",
    "             This data is written to file (the spectrum and the precursor)\n",
    "OPTIONAL:\n",
    "    title_prefix: str name to give as prefix to the name of each spectrum. Default=Spectrum\n",
    "    output_dir: str name of the directory to save files to. Default=./\n",
    "    compress: bool whether or not to compress the file. Comrpesses with gzip. Default=True\n",
    "RETURNS:\n",
    "    list of strings of file paths\n",
    "'''\n",
    "def write_mzml(file_name, spectra, title_prefix='Spectrum ', output_dir='./', compress=True):\n",
    "    if '.mzml' not in file_name.lower():\n",
    "        file_name += '.mzML'\n",
    "    output_dir = __make_valid_dir_string(output_dir)\n",
    "    __make_dir(output_dir)\n",
    "    output_file = output_dir + file_name\n",
    "\n",
    "    exp = MSExperiment()\n",
    "    sp_count = 0\n",
    "    for spectrum in spectra:\n",
    "        spec = MSSpectrum()\n",
    "        spec.setMSLevel(2)\n",
    "        name = str.encode(title_prefix + str(sp_count))\n",
    "        spec.setName(name)\n",
    "        sp_count += 1\n",
    "        \n",
    "        i = [500 for _ in spectrum['spectrum']]\n",
    "        spec.set_peaks([spectrum['spectrum'], i])\n",
    "        spec.setMSLevel(2)\n",
    "        prec = Precursor()\n",
    "        prec.setCharge(2)\n",
    "        prec.setMZ(spectrum['precursor_mass'])\n",
    "        spec.setPrecursors([prec])\n",
    "        spec.sortByPosition()\n",
    "        exp.addSpectrum(spec)\n",
    "\n",
    "    MzMLFile().store(output_file, exp)\n",
    "    output_file = __gzip(output_file) if compress else output_file\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crux scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "\n",
    "crux_to_rm = ['tide-search.decoy.txt', 'tide-search.log.txt', 'tide-search.params.txt']\n",
    "\n",
    "def __parse_spectrum_name(spec_name):\n",
    "    return str(spec_name.split('/')[-1]).lower().replace('.mzml', '')\n",
    "\n",
    "def __parse_db_name(db_name):\n",
    "    return  str(db_name.split('/')[-1]).replace('.fasta', '')\n",
    "\n",
    "def __index_db_files(path_to_crux_cmd, db_files):\n",
    "    idx_names = []\n",
    "    num_dbs = len(db_files)\n",
    "\n",
    "    for i, db_file in enumerate(db_files):\n",
    "        print('On database {}/{}[{}%]\\r'.format(i+1, num_dbs, int(((i+1)/num_dbs) * 100)), end=\"\")\n",
    "        \n",
    "        this_output_dir = '/'.join(str(db_file).split('/')[:-1])\n",
    "        this_output_dir = __make_valid_dir_string(this_output_dir) + 'indexed/'\n",
    "        __make_dir(this_output_dir)\n",
    "        idx_name = this_output_dir + str(db_file).replace('.fasta', '_index').split('/')[-1]\n",
    "\n",
    "        indx_cmd = [\n",
    "            path_to_crux_cmd, \n",
    "            'tide-index', \n",
    "            db_file, \n",
    "            idx_name, \n",
    "            '--min-length', '2', \n",
    "            '--min-mass', '50', \n",
    "            '--output-dir', this_output_dir, \n",
    "            '--overwrite', 'T', \n",
    "            '--min-peaks', '2', \n",
    "            '--precursor-window', '1000000000',\n",
    "            '--enzyme', 'no-enzyme', \n",
    "            '--verbosity', '0'\n",
    "        ]\n",
    "        call(indx_cmd)\n",
    "        # remove extra output files\n",
    "        os.remove(this_output_dir + 'tide-index.params.txt')\n",
    "        os.remove(this_output_dir + 'tide-index.log.txt')\n",
    "\n",
    "        idx_names.append(idx_name)\n",
    "    return idx_names\n",
    "\n",
    "def __remove_indices(index_file):\n",
    "    if isinstance(index_file, list):\n",
    "        index_file = index_file[0]\n",
    "    rm_dir = '/'.join(index_file.split('/')[:-1])\n",
    "    shutil.rmtree(rm_dir)\n",
    "\n",
    "\n",
    "'''crux_search\n",
    "\n",
    "DESC:\n",
    "    use the crux tool to score spectra against databases\n",
    "PARAMS:\n",
    "    spectra_files: list of str paths to all the spectra (.mzML) files\n",
    "    database_files: list of str paths to all the database (.fasta) files\n",
    "    path_to_crux_cmd: str path to the executable for crux.\n",
    "    output_dir: str path to the directory to save files\n",
    "OPTIONAL:\n",
    "    compress: bool compress the output result. Default=True\n",
    "RETURNS:\n",
    "    list of str of output files\n",
    "'''\n",
    "def __crux_search(spectra_files, database_files, path_to_crux_cmd, output_dir, compress=True):\n",
    "    output_dir = __make_valid_dir_string(output_dir) + 'search_output/'\n",
    "    __make_dir(output_dir)\n",
    "    spec_dir = '/'.join(spectra_files[0].split('/')[:-1])\n",
    "\n",
    "    is_compressed = __is_gzipped(spectra_files[0])\n",
    "    print('Pre-indexing database files...')\n",
    "    indexed_db_files = __index_db_files(path_to_crux_cmd, database_files)\n",
    "    print('\\nDone. Scoring..')\n",
    "\n",
    "    output_count = 0\n",
    "    output_files = []\n",
    "    num_dbs = len(indexed_db_files)\n",
    "    num_specs = len(spectra_files)\n",
    "\n",
    "    for i, spec_file in enumerate(spectra_files):\n",
    "        spec_file = spec_file if not is_compressed else __gunzip(spec_file)\n",
    "        for j, database_file in enumerate(indexed_db_files):\n",
    "            this_db_name = __parse_db_name(database_file)\n",
    "            print('On spectrum {}/{}[{}%]\\tOn database file {}/{}[{}%]\\r'.format(i+1, num_specs, int(((i+1)/num_specs) * 100), j+1, num_dbs, int(((j+1)/num_dbs)*100)), end=\"\")\n",
    "            this_output_dir = output_dir + '{}_vs_{}'.format(__parse_spectrum_name(spec_file), this_db_name)\n",
    "            search_cmd = [\n",
    "                path_to_crux_cmd, \n",
    "                'tide-search', \n",
    "                spec_file, \n",
    "                database_file, \n",
    "                '--min-length', '2', \n",
    "                '--min-mass', '50', \n",
    "                '--output-dir', this_output_dir, \n",
    "                '--overwrite', 'T', \n",
    "                '--min-peaks', '2', \n",
    "                '--precursor-window', '1000000000',\n",
    "                '--enzyme', 'no-enzyme', \n",
    "                '--verbosity', '0'\n",
    "                ] \n",
    "            call(search_cmd)\n",
    "            output_count += 1\n",
    "            o = this_output_dir + '/tide-search.target.txt' if not compress else __gzip(this_output_dir + '/tide-search.target.txt')\n",
    "            o_tsv = o.replace('.txt', '.tsv')\n",
    "            os.rename(o, o_tsv)\n",
    "            output_files.append(o_tsv)\n",
    "\n",
    "            # saving space, so should remove the extra stuff\n",
    "            if is_compressed:\n",
    "                for rm in crux_to_rm:\n",
    "                    os.remove(this_output_dir + '/' + rm)\n",
    "\n",
    "        # if the files were compressed, we were trying to save disk space so just remove the mzml files\n",
    "        # is_compressed and os.remove(spec_file)\n",
    "\n",
    "    # is_compressed and os.rmdir(spec_dir)\n",
    "    __remove_indices(indexed_db_files)\n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load crux search results\n",
    "\n",
    "### NOTE: the (int) after a protein name is the start index of the sequence, 1 based not 0, so subtract 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "desired_cols = ['file', 'xcorr score', 'sequence', 'protein id', 'target/decoy', 'xcorr rank']\n",
    "\n",
    "def load_search_result(f):\n",
    "    if __is_gzipped(f):\n",
    "        f = __gunzip(f)\n",
    "    df = pd.read_csv(f, '\\t', header=0)\n",
    "    return df[desired_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add hybrid proteins to fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hyb_prots(fasta_file, prots):\n",
    "    with open(fasta_file, 'a') as o:\n",
    "        o.write('\\n')\n",
    "        c = 0\n",
    "        for p in prots:\n",
    "            if 'hybrid' in p['name'].lower():\n",
    "                o.write('>sp|idsomething{}|{}\\n'.format(c, p['name']))\n",
    "                o.write(p['protein'] + '\\n')\n",
    "                c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# START RUNNING FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "experiment_file = '/Users/zacharymcgrath/Desktop/Experiment output/START SCORING SUM/experiment_data.json'\n",
    "save_dir = '/Users/zacharymcgrath/Desktop/Experiment output/TESTING JN SPECTRA OUTPUT'\n",
    "\n",
    "exp = load_experiment(experiment_file)\n",
    "peptides = exp['experiment_info']['peptides']\n",
    "keyed_peps = {}\n",
    "for pep in peptides:\n",
    "    keyed_peps[pep['peptide_name']] = copy.deepcopy(pep)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate spectra files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "spec_files = []\n",
    "\n",
    "for pep in peptides:\n",
    "    name = pep['peptide_name']\n",
    "    seq = pep['peptide_sequence']\n",
    "    spectra = gen_spectra([seq])\n",
    "    spec_files.append(write_mzml(name, spectra, output_dir=save_dir))\n",
    "#print(spec_files)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run crux search on spectra files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-indexing database files...\n",
      "On database 1/1[100%]\n",
      "Done. Scoring..\n",
      "Donepectrum 12/12[100%]\tOn database file 1/1[100%]\n"
     ]
    }
   ],
   "source": [
    "fasta_file = '/Users/zacharymcgrath/Documents/Layer_Research/Proteomics_Experiments/Database_Experiments/test-protein.fasta'\n",
    "path_to_crux_cmd = '/Users/zacharymcgrath/Documents/Layer_Research/crux/bin/crux'\n",
    "\n",
    "# add hybrid proteins to file before we run search\n",
    "add_hyb_prots(fasta_file, exp['experiment_info']['proteins'])\n",
    "\n",
    "score_files = __crux_search(spec_files, [fasta_file], path_to_crux_cmd, save_dir)\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ok by this point we have files with the scores for every peptide against the one database. We can get their names now and reconstruct this. We need to do the following:\n",
    "1. find out which protein crux said it was from \n",
    "2. Determine if it was the the proper position\n",
    "3. If more than 1 candidate, then filter that out\n",
    "4. construct a list of scores for a protein/peptide pair to see if it properly scored or not. For situations where there was no peptide generated, rank = 0. If the correct position or protein was not found, give a score of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for sf in score_files:\n",
    "    f = sf.replace('.gz', '')\n",
    "    dfs.append(load_search_result(f))\n",
    "    \n",
    "df = pd.concat(dfs, ignore_index=True, sort =False)\n",
    "print('Done')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "\n",
    "def take_name(s):\n",
    "        \n",
    "    rm_num = lambda x: x.split('(')[0]\n",
    "    rm_bar = lambda x: x.split('|')[2]\n",
    "    \n",
    "    return [rm_num(rm_bar(y)) for y in s.split(',')]\n",
    "        \n",
    "\n",
    "for p in peptides:\n",
    "    name = p['peptide_name']\n",
    "    search_results[name] = []\n",
    "    \n",
    "    take_start_pos = lambda s: int(s.split('(')[1].split(')')[0])\n",
    "    for i, row in df.iterrows():\n",
    "        if name in row['file'] and row['target/decoy'] != 'decoy':\n",
    "            search_results[name].append({\n",
    "                'protein': take_name(row['protein id']),\n",
    "                'starting_position': take_start_pos(row['protein id']),\n",
    "                'sequence': row['sequence'],\n",
    "                'peptide_name': name,\n",
    "                'rank': row['xcorr rank']\n",
    "            })\n",
    "print('Done')\n",
    "# print(search_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize by parent protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ps = exp['experiment_info']['proteins']\n",
    "prots = {}\n",
    "for p in ps:\n",
    "    prots[p['name']] = p['sequence'] if 'sequence' in p else p['protein']\n",
    "\n",
    "sorted_results = {}\n",
    "for p in prots: \n",
    "    sorted_results[p] = []\n",
    "\n",
    "for peptide in search_results:\n",
    "    for r in search_results[peptide]:\n",
    "        for prot in r['protein']:\n",
    "            sorted_results[prot].append(r)\n",
    "print('Done')\n",
    "# print(sorted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress results\n",
    "For each protein, we want to know if the peptide associated with that result was scored correctly or not, and update some sort of array. This list should be the length of the protein sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# wrong_parent = 'r'\n",
    "# wrong_loc = 'y'\n",
    "hit = 'g'\n",
    "no_data = 'r'\n",
    "\n",
    "protein_peptide_data = {}\n",
    "\n",
    "for prot in prots:\n",
    "    prot_data = [[] for _ in range(len(prots[prot]))]\n",
    "    for r in sorted_results[prot]:\n",
    "        # get the REAL peptide data\n",
    "        info = keyed_peps[r['peptide_name']]\n",
    "        \n",
    "        if prot in r['protein']:\n",
    "            result_start_idx = int(r['starting_position']) -1\n",
    "            # we have the correct parent protein\n",
    "            if prot == info['parent_name']:\n",
    "                # we have the right starting position\n",
    "                if int(info['start_index']) == result_start_idx: #-1 becuase they use 1 based counting\n",
    "                    prot_data[info['start_index']].append((hit, int(r['rank']), r['peptide_name']))\n",
    "                    \n",
    "# THESE ARE COMMENTED OUT TO REDUCE THE NUMBER OF DATA POINTS WE SHOW\n",
    "#                 else:\n",
    "#                     prot_data[result_start_idx].append((wrong_loc, int(r['rank'])))\n",
    "                    \n",
    "#             # we don't have the correct parent protein\n",
    "#             else:\n",
    "#                 prot_data[result_start_idx].append((wrong_parent, int(r['rank'])))\n",
    "                \n",
    "    protein_peptide_data[prot] = prot_data\n",
    "print('Done')\n",
    "#print(protein_peptide_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peptide cleanup\n",
    "\n",
    "go through every peptide and check the correct parent protein at the correct position and see if it managed to find it. If not give it the no data and a -1. Reduce the number of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/12 peptides did not get correct hit\n"
     ]
    }
   ],
   "source": [
    "def has_correct(prot_data, pos):\n",
    "    rs = prot_data[pos]\n",
    "    return any([r[0] == hit for r in rs])\n",
    "\n",
    "number_missed = 0\n",
    "for pep_name, pep_info in keyed_peps.items():\n",
    "    pos = pep_info['start_index']\n",
    "    corr_parent = pep_info['parent_name']\n",
    "    # if we don't have a point in the right starting position, add a point at it saying its wrong\n",
    "    if not has_correct(protein_peptide_data[corr_parent], pos):\n",
    "        protein_peptide_data[corr_parent][pos].append((no_data, -1))\n",
    "        number_missed += 1\n",
    "    # remove any duplicates from this peptide and take only the highest rank\n",
    "    else:\n",
    "        pep_points = [x for x in protein_peptide_data[corr_parent][pos] if x[2] == pep_name]\n",
    "        pep_points.sort(key=lambda x: x[1])\n",
    "        to_remove = [] if len(pep_points) < 2 else pep_points[1:]\n",
    "        for rm in to_remove:\n",
    "            protein_peptide_data[corr_parent][pos].remove(rm)\n",
    "        \n",
    "print('{}/{} peptides did not get correct hit'.format(number_missed, len(keyed_peps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot these bad bois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On protein 6/6[100%]\r"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import gridspec\n",
    "from math import ceil\n",
    "from statistics import mean\n",
    "\n",
    "max_aas = 75\n",
    "plot_width = 14\n",
    "\n",
    "legend_els = [\n",
    "    Line2D([0], [0], color='r', lw=4, label='wrong parent'),\n",
    "    #Line2D([0], [0], color='y', lw=4, label='right parent wrong location'),\n",
    "    Line2D([0], [0], color='g', lw=4, label='correct')\n",
    "    #Line2D([0], [0], color='c', lw=4, label='correct hit missing')\n",
    "]\n",
    "\n",
    "prot_plot_save_dir = __make_valid_dir_string(save_dir + 'protein_plots/')\n",
    "__make_dir(prot_plot_save_dir)\n",
    "\n",
    "prot_c = 0\n",
    "prot_num = len(protein_peptide_data)\n",
    "for prot in protein_peptide_data:\n",
    "    prot_c += 1\n",
    "    print('On protein {}/{}[{}%]\\r'.format(prot_c, prot_num, int( (float(prot_c)/(prot_num)) * 100)), end=\"\")\n",
    "    num_subplots = ceil(len(prots[prot]) / max_aas)\n",
    "    fig, ax = plt.subplots(num_subplots, figsize=(plot_width, 2.5 * num_subplots))\n",
    "    \n",
    "    \n",
    "    for i in range(num_subplots):\n",
    "        this_axis = ax[i]\n",
    "        # get the data needed for this row\n",
    "        end = min([((i+1) * max_aas), len(prots[prot])])\n",
    "        this_data = protein_peptide_data[prot][i*max_aas:end]\n",
    "        this_label = prots[prot][i*max_aas:end]\n",
    "        for x in range(len(this_data)):\n",
    "            if len(this_data[x]) > 0:\n",
    "                # boxplot this\n",
    "                to_box_plot = [x[1] for x in this_data[x]]\n",
    "                color = 'r' if mean(to_box_plot) < 0 else 'g'\n",
    "                bplot = this_axis.boxplot(to_box_plot, positions=[x])\n",
    "                # color it\n",
    "                for attr in ['boxes', 'whiskers', 'caps', 'fliers', 'medians']:\n",
    "                    for item in bplot[attr]:\n",
    "                        item.set(color=color)\n",
    "                \n",
    "#                 for points in this_data[x]:\n",
    "#                     this_axis.plot([x], [points[1]], marker='o', color=points[0])\n",
    "                    \n",
    "        this_axis.set_xticks([j for j in range(len(this_data))])\n",
    "        this_axis.set_xticklabels(this_label)\n",
    "        this_axis.set_yticks([-2, -1, 0, 1, 2, 3, 4, 5, 6])\n",
    "        this_axis.set_yticklabels(['', 'not found', '0', '1', '2', '3', '4', '5', ''])\n",
    "        y_label = 'sequence:  {} - {}'.format(i * max_aas, end - 1)\n",
    "        this_axis.set_ylabel(y_label, fontsize=8)\n",
    "        this_axis.legend(handles=legend_els, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    fig.suptitle(prot)\n",
    "    plot_save_name = prot_plot_save_dir + prot\n",
    "    fig.savefig(plot_save_name)\n",
    "    #fig.show()\n",
    "    plt.close()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
